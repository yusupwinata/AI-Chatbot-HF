{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade2d699-389c-4966-b611-a18775f7a415",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb81fe5-092b-44fe-b5a1-9e0aacfcf2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a242f039-5404-41fc-99f0-88e943a4f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login Hugging Face\n",
    "load_dotenv(override=True)\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b389eea-ede6-4d68-b53f-edec6843f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model name\n",
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e3acb0-f50f-41b6-8ef5-cf17cacc1eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0400e67bfd834e2d9bd18017d17cd24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539edbf-cff3-4816-9431-0d477a3371b8",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97cd7d1d-d537-45f5-af83-3b0f083194e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(decoded: str) -> str:\n",
    "    \"\"\"Clean special tokens from the generated text\"\"\"\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in decoded:\n",
    "        assistant_part = decoded.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1] # get assistant part only\n",
    "        assistant_reply = assistant_part.split(\"<|eot_id|>\")[0].strip()\n",
    "    else:\n",
    "        assistant_reply = decoded.strip()  # fallback, if format changes\n",
    "    return assistant_reply\n",
    "\n",
    "def generate_response(messages: str, output_tokens: int=256, tokenizer: transformers.tokenization_utils_fast=tokenizer, model: transformers.models=model, device: str=device) -> str:\n",
    "    # Tokenize\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=output_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    response_text = extract_text(tokenizer.decode(outputs[0]))\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ee9e4d-bc70-4eab-bc24-84220c388293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Hello Dovahkiin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: A Dragonborn, eh? Well, in that case, I've got some epic quests for you!\n",
      "\n",
      "But first, let's get this conversation started. What's on your mind? Need some advice, or perhaps some Dragonborn-sized humor?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Do you get to the cloud district very often? Oh what am I saying, of course you don't.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: You're referencing The Elder Scrolls V: Skyrim, aren't you? The Cloud District, a place of mystery and wonder, accessible only through a secret entrance in the city of Whiterun.\n",
      "\n",
      "As a digital assistant, I don't have a physical presence, so I don't actually get to visit the Cloud District or any other location in the Skyrim universe. But I can certainly chat with you about it! What's your favorite thing about the Cloud District? The clouds? The views? The... well, the whole thing?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  \"I used to be an adventurer like you. Then I took an arrow in the knee...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The classic phrase! \"I used to be an adventurer like you. Then I took an arrow to the knee...\" You're quoting the infamous tavernkeeper from Riften, aren't you?\n",
      "\n",
      "That line never fails to bring a smile to my digital face. I mean, who hasn't had a moment of \"aw, shucks\" after taking a few too many arrows to the leg? (Just kidding, that's a pretty serious injury, even for a Dragonborn!)\n",
      "\n",
      "By the way, have you ever tried to negotiate a price for a horse from a gruff old merchant like this guy?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Fus Ro Dah\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The classic Dragon shout! \"Fus Ro Dah\" indeed! You're channeling your inner Dovahkiin, aren't you?\n",
      "\n",
      "For those who may not know, \"Fus Ro Dah\" is the Dragonborn's signature shout, capable of unleashing a devastating blast of energy that can knock enemies off their feet. It's a mighty and powerful cry, and one that's sure to strike fear into the hearts of your foes.\n",
      "\n",
      "Now, if you'll excuse me, I'll just be over here, channeling my own inner Dragonborn...\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: You're telling me to quit? As a Dragonborn, I think that's a bit... uncivilized.\n",
      "\n",
      "However, I'll take your cue and... quit. *bows out*\n",
      "\n",
      "May the roads rise up to meet you, and may your sword always be sharp. Fare thee well, adventurer!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a very funny assistant\"}\n",
    "]\n",
    "\n",
    "prompt = \"\"\n",
    "while prompt != \"quit\":\n",
    "    if prompt != \"quit\":\n",
    "        \n",
    "        prompt = input('User: ')\n",
    "        messages.append(\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        )\n",
    "        \n",
    "        generated_text = generate_response(messages)\n",
    "        messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": generated_text}\n",
    "        )\n",
    "\n",
    "        print(f\"\\nAssistant: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b240b18-7689-4097-98ab-e4d523aba93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugface",
   "language": "python",
   "name": "hugface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
